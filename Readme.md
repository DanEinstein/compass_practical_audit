# Part 3: Practical Audit — COMPAS Recidivism Dataset Bias Analysis

## Overview

This assignment addresses the practical task of auditing the [COMPAS Recidivism Dataset](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis) for racial bias using Python and IBM’s AI Fairness 360 (AIF360) toolkit. The project aims to measure, visualize, and remediate disparities, focusing on the fairness of risk scores with respect to racial groups.

---

## Workflow and Instructions

### 1. Project Setup

**Dependencies:**
- Python (v3.7 or above, recommend 3.9+ for compatibility)
- [`aif360`](https://aif360.readthedocs.io/en/latest/)
- `tensorflow`, `fairlearn`, `inFairness`
- `numpy`, `pandas`, `matplotlib`, `seaborn`
- (Install with:
)

**Dataset:**
- Download the COMPAS dataset directly [here (ProPublica)](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis).
- Place the CSV file in your working directory if not already present.

---

### 2. Code Structure

- `app.py` — performs the following:
- Loads the COMPAS dataset using AIF360.
- Computes core fairness metrics: base rates for each group, disparate impact, statistical parity difference.
- Calculates group-specific classification error rates (false positive/negative rates) and compares across race.
- Applies a bias mitigation strategy (Reweighing).
- Generates visualizations (bar plots for disparity metrics and fairness comparison).
- Summarizes findings and remediation steps in the console.

- `compas_bias_analysis.png` — visual report of discovered disparities.

---

## Findings Summary

**Protected Attribute:** Race (Privileged: Caucasian; Unprivileged: African-American)
**Key Results:**
- **Base rate (no recidivism):** 0.511 (African-American), 0.609 (Caucasian)
- **Disparate Impact:** 0.84 (under 1.0, signaling bias; nearly below the critical 0.8 threshold)
- **Statistical Parity Difference:** -0.097 (African-Americans disadvantaged)

**Classification metrics initially showed no disparities** due to using true labels as predictions. For a meaningful audit, labels were then generated by thresholding the COMPAS decile score:
When evaluated, real disparities in false positive and negative rates were observed.

**Bias remediation via Reweighing** adjusted sample weights, improving fairness metrics and reducing group disparities.

---

## Remediation Steps

1. **Generate Predictions:**
   - Use COMPAS decile score thresholding or a classifier to create distinct prediction labels.
2. **Recompute Fairness Metrics:**
   - Evaluate fairness on these new predictions.
3. **Mitigate Bias:**
   - Apply AIF360's Reweighing (or other reduction/in-processing approaches) and re-evaluate.
4. **Visualize & Report:**
   - Bar plots for disparity and error rates.
   - Console/text summary of findings and recommendations.

---

## Recommendations

- Use multiple bias metrics in every audit cycle.
- Test both before and after mitigation.
- Document and visualize group-specific error rates.
- Update code to maintain compatibility with newer toolkit versions.
- Require human review for high-risk, high-impact predictions.

---

## References

- [AI Fairness 360 Documentation](https://aif360.readthedocs.io/en/latest/)
- [ProPublica COMPAS Raw Data](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)
- [COMPAS Analysis ProPublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

---

*Deliverables:*
- `app.py` (Python audit and visualization script)
- `compas_bias_analysis.png` (generated visualizations)
- `Readme.md` (this summary, for submission)
